services:
  # ============ STORAGE ============
  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - projecto-final_default

  # Crea bucket para MLflow en MinIO
  minio-mc:
    image: minio/mc:latest
    container_name: minio-mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
        mc alias set minio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} &&
        mc mb -p minio/mlflow || true &&
        mc policy set public minio/mlflow || true
      "
    restart: "no"
    networks:
        - projecto-final_default

  # ============ MLflow DB ============
  mlflow-db:
    image: postgres:16
    container_name: mlflow-db
    environment:
      POSTGRES_DB: ${MLFLOW_DB}
      POSTGRES_USER: ${MLFLOW_DB_USER}
      POSTGRES_PASSWORD: ${MLFLOW_DB_PASSWORD}
    volumes:
      - mlflow_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${MLFLOW_DB_USER} -d ${MLFLOW_DB}"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - projecto-final_default

  # ============ MLflow SERVER ============
  mlflow:
    build:
        context: .
        dockerfile: Dockerfile.mlflow
    container_name: mlflow
    depends_on:
      mlflow-db:
        condition: service_healthy
      minio:
        condition: service_healthy
      minio-mc:
        condition: service_completed_successfully
    ports:
      - "5000:5000"
    environment:
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL:-http://minio:9000}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-${MINIO_ROOT_USER}}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-${MINIO_ROOT_PASSWORD}}
    command: >
      mlflow server
      --backend-store-uri postgresql+psycopg2://${MLFLOW_DB_USER}:${MLFLOW_DB_PASSWORD}@mlflow-db:5432/${MLFLOW_DB_NAME}
      --default-artifact-root s3://mlflow/
      --host 0.0.0.0
      --port 5000
    networks:
      - projecto-final_default

  # ============ RAW POSTGRES ============
  raw-db:
    image: postgres:16
    container_name: raw-db
    environment:
      POSTGRES_DB: ${RAW_DB}
      POSTGRES_USER: ${RAW_USER}
      POSTGRES_PASSWORD: ${RAW_PASSWORD}
    volumes:
      - raw_db_data:/var/lib/postgresql/data
      - ./postgresql/raw/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${RAW_USER} -d ${RAW_DB}"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - projecto-final_default

  # ============ CLEAN POSTGRES ============
  clean-db:
    image: postgres:16
    container_name: clean-db
    environment:
      POSTGRES_DB: ${CLEAN_DB}
      POSTGRES_USER: ${CLEAN_USER}
      POSTGRES_PASSWORD: ${CLEAN_PASSWORD}
    volumes:
      - clean_db_data:/var/lib/postgresql/data
      - ./postgresql/clean/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${CLEAN_USER} -d ${CLEAN_DB}"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - projecto-final_default

  # ============ AIRFLOW DB ============
  airflow-db:
    image: postgres:16
    container_name: airflow-db
    environment:
      POSTGRES_DB: airflow
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - projecto-final_default

  # ============ AIRFLOW INIT ============
  airflow-init:
    image: apache/airflow:2.9.3
    container_name: airflow-init
    entrypoint: /bin/bash
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR:-SequentialExecutor}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES:-False}
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: "False"
      PYTHONWARNINGS: ignore::SyntaxWarning
      _PIP_ADDITIONAL_REQUIREMENTS: "-r /opt/airflow/requirements.txt"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
    command:
      - -lc
      - |
        set -e

        echo '[INIT] Waiting for database to be ready...'
        MAX_RETRIES=30
        RETRY=0
        until airflow db check > /dev/null 2>&1; do
          RETRY=$((RETRY + 1))
          if [ $RETRY -ge $MAX_RETRIES ]; then
            echo "❌ Database not ready after $MAX_RETRIES retries"
            exit 1
          fi
          echo "Database not ready yet. Waiting... (attempt $RETRY/$MAX_RETRIES)"
          sleep 2
        done
        echo "✅ Database is ready!"

        echo '[INIT] airflow db init'
        airflow db init || echo "⚠️  Database already initialized (this is OK)"

        echo '[INIT] create or reset admin user'
        set +e 
        airflow users create \
          --username "${AIRFLOW_ADMIN_USER:-admin}" \
          --password "${AIRFLOW_ADMIN_PASSWORD:-admin123}" \
          --firstname Admin --lastname User --role Admin \
          --email "${AIRFLOW_ADMIN_EMAIL:-admin@example.com}" || echo "⚠️  User already exists (this is OK)"
        airflow users update-password \
          --username "${AIRFLOW_ADMIN_USER:-admin}" \
          --password "${AIRFLOW_ADMIN_PASSWORD:-admin123}" || echo "⚠️  Password update skipped (this is OK)"
        set -e

        echo '[INIT] add connections'
        airflow connections delete postgres_raw   || true
        airflow connections delete postgres_clean || true
        airflow connections delete minio_s3       || true
        airflow connections delete mlflow_tracking || true

        airflow connections add postgres_raw --conn-type postgres \
          --conn-host "${RAW_HOST:-raw-db}" \
          --conn-login "${RAW_USER}" \
          --conn-password "${RAW_PASSWORD}" \
          --conn-schema "${RAW_DB}" \
          --conn-port 5432 || echo "⚠️  Failed to add postgres_raw connection"

        airflow connections add postgres_clean --conn-type postgres \
          --conn-host "${CLEAN_HOST:-clean-db}" \
          --conn-login "${CLEAN_USER}" \
          --conn-password "${CLEAN_PASSWORD}" \
          --conn-schema "${CLEAN_DB}" \
          --conn-port 5432 || echo "⚠️  Failed to add postgres_clean connection"

        airflow connections add minio_s3 --conn-type s3 \
          --conn-login "${MINIO_ROOT_USER}" \
          --conn-password "${MINIO_ROOT_PASSWORD}" \
          --conn-extra '{"endpoint_url":"http://minio:9000"}' || echo "⚠️  Failed to add minio_s3 connection"

        airflow connections add mlflow_tracking --conn-type http \
          --conn-host mlflow \
          --conn-port 5000 || echo "⚠️  Failed to add mlflow_tracking connection"

        echo '[INIT] ✅ done.'
    volumes:
      - ./airflow:/opt/airflow
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt:ro
    networks:
      - projecto-final_default


  # ============ AIRFLOW WEBSERVER ============
  airflow-webserver:
    image: apache/airflow:2.9.2
    container_name: airflow-webserver
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    env_file:
        - .env
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID:-50000}
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      _PIP_ADDITIONAL_REQUIREMENTS: "-r /opt/airflow/requirements.txt"
      PYTHONWARNINGS: ignore::SyntaxWarning
    volumes:
      - ./airflow:/opt/airflow
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt:ro
    ports:
      - "8080:8080"
    command: webserver
    networks:
      - projecto-final_default

  # ============ AIRFLOW SCHEDULER ============
  airflow-scheduler:
    image: apache/airflow:2.9.2
    container_name: airflow-scheduler
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    env_file:
        - .env
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID:-50000}
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      _PIP_ADDITIONAL_REQUIREMENTS: "-r /opt/airflow/requirements.txt"
      PYTHONWARNINGS: ignore::SyntaxWarning
    volumes:
      - ./airflow:/opt/airflow
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt:ro
    command: scheduler
    networks:
      - projecto-final_default

volumes:
  mlflow_db_data:
  raw_db_data:
  clean_db_data:
  minio_data:
  airflow_db_data:
  
networks:
  projecto-final_default:
    driver: bridge
