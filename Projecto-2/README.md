# Proyecto 2 - Airflow ML Pipeline

Este proyecto implementa un pipeline de Machine Learning usando Apache Airflow que obtiene datos de una API externa, los procesa y entrena un modelo de clasificaciÃ³n.

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos
- Docker y Docker Compose instalados
- Puerto 8080 disponible

### ConfiguraciÃ³n

1. **Copia el archivo `.env.example` a `.env`** en el directorio raÃ­z

2. **Ejecutar el pipeline**:
```bash
cd Projecto-2
docker-compose up --build
```

3. **Acceder a Airflow**:
- URL: http://localhost:8080
- Usuario: `admin`
- ContraseÃ±a: `admin123`

## ğŸ“Š Pipeline de ML

### DAG: `p2_covertype_single_request`

**ProgramaciÃ³n**: Cada 5 minutos
**DescripciÃ³n**: Pipeline que procesa datos de cobertura forestal

#### Tareas:
1. **fetch_once**: Obtiene datos de la API externa
2. **validate_and_preprocess**: Valida y preprocesa los datos
3. **train_eval**: Entrena modelo RandomForest y calcula mÃ©tricas

### ConfiguraciÃ³n del Pipeline

Las variables del pipeline se configuran automÃ¡ticamente:

| Variable | Valor | DescripciÃ³n |
|----------|-------|-------------|
| `P2_API_BASE` | `http://10.43.100.103:80` | URL base de la API |
| `P2_GROUP_ID` | `3` | ID del grupo |
| `P2_API_PATH` | `/data` | Endpoint de datos |
| `P2_TARGET_COL` | `Cover_Type` | Columna objetivo |
| `P2_SCHEDULE_CRON` | `*/5 * * * *` | Ejecutar cada 5 minutos |

## ğŸ› ï¸ Estructura del Proyecto

```
Projecto-2/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ p2_covertype_single_request.py  # DAG principal
â”‚   â”œâ”€â”€ Dockerfile                          # Imagen custom de Airflow
â”‚   â””â”€â”€ requirements.txt                    # Dependencias Python
â”œâ”€â”€ docker-compose.yml                      # ConfiguraciÃ³n de servicios
â”œâ”€â”€ .env                                    # Variables de entorno
â””â”€â”€ README.md                               # Este archivo
```

## ğŸ“ˆ Resultados

Los resultados del pipeline se guardan en `/opt/airflow/data/p2/` dentro del contenedor:

- **Datos raw**: `raw_{timestamp}.json`
- **Datos procesados**: `X_{timestamp}.parquet`, `y_{timestamp}.parquet`
- **MÃ©tricas**: `metrics_{timestamp}.json`
- **Modelo**: `model_{timestamp}.joblib`

## ğŸ”§ Comandos Ãštiles

### Detener servicios
```bash
docker-compose down
```

### Ver logs
```bash
docker-compose logs airflow-webserver
docker-compose logs airflow-scheduler
```

### Reconstruir imÃ¡genes
```bash
docker-compose up --build --force-recreate
```

## ğŸ“ Notas

- El DAG se activa automÃ¡ticamente al iniciar
- Los datos se procesan y persisten localmente en el contenedor
- Las mÃ©tricas del modelo se imprimen en los logs de Airflow
- El pipeline maneja errores bÃ¡sicos de validaciÃ³n de datos