{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêß Experimentos MLflow - Palmer Penguins\n",
    "\n",
    "Este notebook implementa:\n",
    "1. Verificaci√≥n de servicios y bucket S3\n",
    "2. Carga de datos en MySQL (penguins_raw)\n",
    "3. Preprocesamiento y generaci√≥n de datos limpios (penguins_clean)\n",
    "4. ‚â•20 experimentos con diferentes modelos e hiperpar√°metros\n",
    "5. Registro del mejor modelo en MLflow Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Patch aplicado para compatibilidad con SQLAlchemy 2.0\n"
     ]
    }
   ],
   "source": [
    "# Fix de compatibilidad para SQLAlchemy 2.0\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text as sql_text\n",
    "\n",
    "# Monkey patch para compatibilidad\n",
    "original_execute = sqlalchemy.engine.Connection.execute\n",
    "\n",
    "def patched_execute(self, statement, *args, **kwargs):\n",
    "    if isinstance(statement, str):\n",
    "        statement = sql_text(statement)\n",
    "    return original_execute(self, statement, *args, **kwargs)\n",
    "\n",
    "sqlalchemy.engine.Connection.execute = patched_execute\n",
    "\n",
    "print(\"‚úÖ Patch aplicado para compatibilidad con SQLAlchemy 2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: http://mlflow:5000\n"
     ]
    }
   ],
   "source": [
    "# Importaciones necesarias\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine, text\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000'))\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Verificar configuraci√≥n y crear bucket si es necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bucket 'mlflows3' existe y est√° accesible\n"
     ]
    }
   ],
   "source": [
    "# Verificar y crear bucket S3 si es necesario\n",
    "def verify_s3_bucket():\n",
    "    \"\"\"Verifica que el bucket mlflows3 existe, intenta crearlo si no existe\"\"\"\n",
    "    try:\n",
    "        # Configurar cliente S3\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=os.getenv('MLFLOW_S3_ENDPOINT_URL', 'http://minio:9000'),\n",
    "            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID', 'admin'),\n",
    "            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY', 'supersecret'),\n",
    "            region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n",
    "        )\n",
    "        \n",
    "        # Verificar si el bucket existe\n",
    "        try:\n",
    "            s3_client.head_bucket(Bucket='mlflows3')\n",
    "            print(\"‚úÖ Bucket 'mlflows3' existe y est√° accesible\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            error_code = int(e.response['Error']['Code'])\n",
    "            if error_code == 404:\n",
    "                print(\"‚ö†Ô∏è  Bucket 'mlflows3' no existe. Intentando crear...\")\n",
    "                try:\n",
    "                    s3_client.create_bucket(Bucket='mlflows3')\n",
    "                    print(\"‚úÖ Bucket 'mlflows3' creado exitosamente\")\n",
    "                    return True\n",
    "                except Exception as create_error:\n",
    "                    print(f\"‚ùå Error al crear bucket: {create_error}\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\"‚ùå Error al verificar bucket: {e}\")\n",
    "                return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error de conexi√≥n S3: {e}\")\n",
    "        return False\n",
    "\n",
    "# Verificar bucket antes de continuar\n",
    "bucket_ready = verify_s3_bucket()\n",
    "\n",
    "if not bucket_ready:\n",
    "    print(\"\\n‚ö†Ô∏è  ADVERTENCIA: El bucket S3 no est√° disponible.\")\n",
    "    print(\"Los experimentos pueden fallar al guardar artefactos.\")\n",
    "    print(\"Intenta ejecutar en una terminal:\")\n",
    "    print(\"docker exec mlflow-minio mc mb myminio/mlflows3\")\n",
    "    print(\"docker exec mlflow-minio mc anonymous set download myminio/mlflows3\")\n",
    "    print(\"\\nEsperando 30 segundos antes de continuar...\")\n",
    "    time.sleep(30)\n",
    "    # Intentar una vez m√°s\n",
    "    bucket_ready = verify_s3_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de conexi√≥n a MySQL\n",
    "MYSQL_CONFIG = {\n",
    "    'host': os.getenv('MYSQL_HOST', 'mysql'),\n",
    "    'port': int(os.getenv('MYSQL_PORT', 3306)),\n",
    "    'user': os.getenv('MYSQL_USER', 'penguins'),\n",
    "    'password': os.getenv('MYSQL_PASSWORD', 'penguins123'),\n",
    "    'database': os.getenv('MYSQL_DATABASE', 'penguins_db')\n",
    "}\n",
    "\n",
    "# Crear engine de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{MYSQL_CONFIG['user']}:{MYSQL_CONFIG['password']}@\"\n",
    "    f\"{MYSQL_CONFIG['host']}:{MYSQL_CONFIG['port']}/{MYSQL_CONFIG['database']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar datos crudos en MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado desde palmerpenguins\n",
      "Shape: (344, 8)\n",
      "Columnas: ['species', 'island', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex', 'year']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  year  \n",
       "0       3750.0    male  2007  \n",
       "1       3800.0  female  2007  \n",
       "2       3250.0  female  2007  \n",
       "3          NaN     NaN  2007  \n",
       "4       3450.0  female  2007  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar dataset de penguins\n",
    "try:\n",
    "    from palmerpenguins import load_penguins\n",
    "    df_raw = load_penguins()\n",
    "    print(\"Dataset cargado desde palmerpenguins\")\n",
    "except:\n",
    "    import seaborn as sns\n",
    "    df_raw = sns.load_dataset('penguins')\n",
    "    print(\"Dataset cargado desde seaborn\")\n",
    "\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(f\"Columnas: {df_raw.columns.tolist()}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#MOD\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Insertar datos crudos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf_raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpenguins_raw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úì \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_raw)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m registros insertados en penguins_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:3008\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[1;32m   2815\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3004\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[1;32m   3005\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[0;32m-> 3008\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/sql.py:788\u001b[0m, in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    785\u001b[0m     )\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema, need_transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/sql.py:1958\u001b[0m, in \u001b[0;36mSQLDatabase.to_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m sql_engine \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m   1948\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_table(\n\u001b[1;32m   1949\u001b[0m     frame\u001b[38;5;241m=\u001b[39mframe,\n\u001b[1;32m   1950\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1955\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1956\u001b[0m )\n\u001b[0;32m-> 1958\u001b[0m total_inserted \u001b[38;5;241m=\u001b[39m \u001b[43msql_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_records\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_case_sensitive(name\u001b[38;5;241m=\u001b[39mname, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_inserted\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/sql.py:1498\u001b[0m, in \u001b[0;36mSQLAlchemyEngine.insert_records\u001b[0;34m(self, table, con, frame, name, index, schema, chunksize, method, **engine_kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exc\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1498\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mStatementError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;66;03m# GH34431\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;66;03m# https://stackoverflow.com/a/67358288/6067848\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(1054, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown column \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf(e0)?\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield list\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m))(?#\u001b[39m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124m    )|inf can not be used with MySQL\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/sql.py:1059\u001b[0m, in \u001b[0;36mSQLTable.insert\u001b[0;34m(self, chunksize, method)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m chunk_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(arr[start_i:end_i] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m data_list))\n\u001b[0;32m-> 1059\u001b[0m num_inserted \u001b[38;5;241m=\u001b[39m \u001b[43mexec_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# GH 46891\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_inserted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/sql.py:951\u001b[0m, in \u001b[0;36mSQLTable._execute_insert\u001b[0;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;124;03mExecute SQL statement inserting data\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m   Each item contains a list of values to be inserted\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    950\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(keys, row)) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data_iter]\n\u001b[0;32m--> 951\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mrowcount\n",
      "Cell \u001b[0;32mIn[54], line 11\u001b[0m, in \u001b[0;36mpatched_execute\u001b[0;34m(self, statement, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(statement, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     10\u001b[0m     statement \u001b[38;5;241m=\u001b[39m sql_text(statement)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 11\u001b[0m, in \u001b[0;36mpatched_execute\u001b[0;34m(self, statement, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(statement, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     10\u001b[0m     statement \u001b[38;5;241m=\u001b[39m sql_text(statement)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: patched_execute at line 11 (2964 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[54], line 11\u001b[0m, in \u001b[0;36mpatched_execute\u001b[0;34m(self, statement, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(statement, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     10\u001b[0m     statement \u001b[38;5;241m=\u001b[39m sql_text(statement)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "#MOD\n",
    "# Insertar datos crudos\n",
    "df_raw.to_sql('penguins_raw', engine, if_exists='append', index=False)\n",
    "print(f\"‚úì {len(df_raw)} registros insertados en penguins_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesar datos y generar penguins_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer datos desde MySQL\n",
    "df_from_db = pd.read_sql(\"SELECT * FROM penguins_raw\", engine)\n",
    "print(f\"Datos le√≠dos: {df_from_db.shape}\")\n",
    "\n",
    "# Eliminar filas con valores nulos en features cr√≠ticas\n",
    "critical_features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'species']\n",
    "df_clean = df_from_db.dropna(subset=critical_features)\n",
    "print(f\"Despu√©s de eliminar nulos: {df_clean.shape}\")\n",
    "\n",
    "# Codificar variable objetivo\n",
    "label_encoder = LabelEncoder()\n",
    "df_clean['species_encoded'] = label_encoder.fit_transform(df_clean['species'])\n",
    "species_mapping = {i: sp for i, sp in enumerate(label_encoder.classes_)}\n",
    "print(f\"Mapeo de especies: {species_mapping}\")\n",
    "\n",
    "# Rellenar valores faltantes opcionales\n",
    "df_clean['sex'] = df_clean['sex'].fillna('Unknown')\n",
    "df_clean['year'] = df_clean['year'].fillna(df_clean['year'].median())\n",
    "\n",
    "# Agregar timestamp\n",
    "df_clean['processed_at'] = pd.Timestamp.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOD\n",
    "df_clean.to_sql('penguins_clean', engine, if_exists='append', index=False)\n",
    "print(f\"‚úì {len(df_clean)} registros insertados en penguins_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparar datos para Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar features y target\n",
    "feature_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "X = df_clean[feature_cols].values\n",
    "y = df_clean['species_encoded'].values\n",
    "\n",
    "# Divisi√≥n train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Distribuci√≥n de clases en train: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimentos con MLflow (‚â•20 runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear experimento en MLflow\n",
    "experiment_name = \"penguins-classification\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Obtener ID del experimento\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"Experimento: {experiment_name} (ID: {experiment.experiment_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para entrenar modelo con manejo de errores\n",
    "def train_and_log_model(model, model_name, run_name, params, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Entrena un modelo y lo registra en MLflow con manejo de errores\"\"\"\n",
    "    try:\n",
    "        with mlflow.start_run(run_name=run_name):\n",
    "            # Log parameters\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_param(\"model_type\", model_name)\n",
    "            \n",
    "            # Entrenar modelo\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predicciones\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # M√©tricas\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            \n",
    "            # Intentar log model con reintentos\n",
    "            max_retries = 3\n",
    "            for retry in range(max_retries):\n",
    "                try:\n",
    "                    mlflow.sklearn.log_model(\n",
    "                        model, \n",
    "                        \"model\",\n",
    "                        input_example=X_train[:1],\n",
    "                        signature=mlflow.models.infer_signature(X_train, y_train)\n",
    "                    )\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if \"NoSuchBucket\" in str(e) and retry < max_retries - 1:\n",
    "                        print(f\"‚ö†Ô∏è  Error con bucket S3, reintentando en 5s... ({retry+1}/{max_retries})\")\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        print(f\"‚ùå Error al guardar modelo: {e}\")\n",
    "                        print(\"   El modelo se entren√≥ pero no se pudo guardar en S3\")\n",
    "            \n",
    "            print(f\"{run_name}: Accuracy={accuracy:.4f}, F1={f1:.4f}\")\n",
    "            return accuracy, f1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en {run_name}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos 1-5: Random Forest con diferentes hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones de Random Forest\n",
    "rf_configs = [\n",
    "    {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5},\n",
    "    {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2},\n",
    "    {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2},\n",
    "    {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 4},\n",
    "    {'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 3}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(rf_configs, 1):\n",
    "    rf = RandomForestClassifier(random_state=42, **config)\n",
    "    train_and_log_model(\n",
    "        rf, \n",
    "        \"RandomForest\", \n",
    "        f\"rf_experiment_{i}\", \n",
    "        config,\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos 6-10: KNN con diferentes valores de k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN experiments\n",
    "k_values = [3, 5, 7, 10, 15]\n",
    "weights_options = ['uniform', 'distance']\n",
    "\n",
    "exp_num = 6\n",
    "for k in k_values:\n",
    "    for weights in weights_options[:1]:  # Solo 'uniform' para llegar a 5 experimentos\n",
    "        # Crear pipeline con escalado\n",
    "        knn_pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=k, weights=weights))\n",
    "        ])\n",
    "        \n",
    "        params = {'n_neighbors': k, 'weights': weights}\n",
    "        train_and_log_model(\n",
    "            knn_pipeline,\n",
    "            \"KNN\",\n",
    "            f\"knn_experiment_{exp_num}\",\n",
    "            params,\n",
    "            X_train, y_train, X_test, y_test\n",
    "        )\n",
    "        exp_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos 11-15: SVM con diferentes kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:28 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_experiment_11: Accuracy=1.0000, F1=1.0000\n",
      "üèÉ View run svm_experiment_11 at: http://mlflow:5000/#/experiments/1/runs/ed95d5420eb34c00a55a365d478becb5\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_experiment_12: Accuracy=1.0000, F1=1.0000\n",
      "üèÉ View run svm_experiment_12 at: http://mlflow:5000/#/experiments/1/runs/fe3760c59f9d4dd8af4eba2a51690e3a\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_experiment_13: Accuracy=1.0000, F1=1.0000\n",
      "üèÉ View run svm_experiment_13 at: http://mlflow:5000/#/experiments/1/runs/5fd701d11ccc4c44a017695f74613488\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:37 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_experiment_14: Accuracy=1.0000, F1=1.0000\n",
      "üèÉ View run svm_experiment_14 at: http://mlflow:5000/#/experiments/1/runs/50567bfe44044e61bfa6db5ff380ee7a\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_experiment_15: Accuracy=0.9565, F1=0.9441\n",
      "üèÉ View run svm_experiment_15 at: http://mlflow:5000/#/experiments/1/runs/1928b2c987024e0bb31b05c5c83f7cc9\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# SVM experiments\n",
    "svm_configs = [\n",
    "    {'kernel': 'linear', 'C': 0.1},\n",
    "    {'kernel': 'linear', 'C': 1.0},\n",
    "    {'kernel': 'rbf', 'C': 1.0, 'gamma': 'scale'},\n",
    "    {'kernel': 'rbf', 'C': 10.0, 'gamma': 'auto'},\n",
    "    {'kernel': 'poly', 'C': 1.0, 'degree': 3}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(svm_configs, 11):\n",
    "    # Crear pipeline con escalado\n",
    "    svm_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(probability=True, random_state=42, **config))\n",
    "    ])\n",
    "    \n",
    "    train_and_log_model(\n",
    "        svm_pipeline,\n",
    "        \"SVM\",\n",
    "        f\"svm_experiment_{i}\",\n",
    "        config,\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos 16-20: XGBoost con diferentes configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:43 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost_experiment_16: Accuracy=0.9565, F1=0.9528\n",
      "üèÉ View run xgboost_experiment_16 at: http://mlflow:5000/#/experiments/1/runs/52d8904a7c254a6e81c1fc2ae1d7b9c5\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost_experiment_17: Accuracy=0.9565, F1=0.9528\n",
      "üèÉ View run xgboost_experiment_17 at: http://mlflow:5000/#/experiments/1/runs/496f0b19a7cc4d6ebd86ff997d09ef4a\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost_experiment_18: Accuracy=0.9710, F1=0.9692\n",
      "üèÉ View run xgboost_experiment_18 at: http://mlflow:5000/#/experiments/1/runs/39767c3e10854c03bd3bacce94a12e01\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost_experiment_19: Accuracy=0.9710, F1=0.9653\n",
      "üèÉ View run xgboost_experiment_19 at: http://mlflow:5000/#/experiments/1/runs/6abba55f05e24ea889daabeef6c6907d\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost_experiment_20: Accuracy=0.9710, F1=0.9653\n",
      "üèÉ View run xgboost_experiment_20 at: http://mlflow:5000/#/experiments/1/runs/f608a189ac944f9ca59cdd6709195443\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# XGBoost experiments\n",
    "xgb_configs = [\n",
    "    {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1},\n",
    "    {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.05},\n",
    "    {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.01},\n",
    "    {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.1},\n",
    "    {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.3}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(xgb_configs, 16):\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        random_state=42,\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    train_and_log_model(\n",
    "        xgb_model,\n",
    "        \"XGBoost\",\n",
    "        f\"xgboost_experiment_{i}\",\n",
    "        config,\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos adicionales 21-25: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:44:59 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm_experiment_21: Accuracy=0.9565, F1=0.9528\n",
      "üèÉ View run lightgbm_experiment_21 at: http://mlflow:5000/#/experiments/1/runs/b7c17ba2519d473e8febb6719c6ee3f3\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:45:02 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm_experiment_22: Accuracy=0.9710, F1=0.9653\n",
      "üèÉ View run lightgbm_experiment_22 at: http://mlflow:5000/#/experiments/1/runs/05a1f1672e9f4e13aa0baf914d7ed525\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:45:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm_experiment_23: Accuracy=0.9565, F1=0.9528\n",
      "üèÉ View run lightgbm_experiment_23 at: http://mlflow:5000/#/experiments/1/runs/d4b11c51b3a84459abc5bce57d7a9205\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:45:10 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm_experiment_24: Accuracy=0.9855, F1=0.9817\n",
      "üèÉ View run lightgbm_experiment_24 at: http://mlflow:5000/#/experiments/1/runs/dc16b67ba1a84061918f272c2f38b446\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:45:14 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm_experiment_25: Accuracy=0.9710, F1=0.9653\n",
      "üèÉ View run lightgbm_experiment_25 at: http://mlflow:5000/#/experiments/1/runs/fecb4aa30ef648dca2b07f9b723353f4\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# LightGBM experiments para completar ‚â•20\n",
    "lgb_configs = [\n",
    "    {'n_estimators': 100, 'num_leaves': 31, 'learning_rate': 0.1},\n",
    "    {'n_estimators': 200, 'num_leaves': 50, 'learning_rate': 0.05},\n",
    "    {'n_estimators': 150, 'num_leaves': 20, 'learning_rate': 0.15},\n",
    "    {'n_estimators': 300, 'num_leaves': 40, 'learning_rate': 0.01},\n",
    "    {'n_estimators': 250, 'num_leaves': 60, 'learning_rate': 0.08}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(lgb_configs, 21):\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    train_and_log_model(\n",
    "        lgb_model,\n",
    "        \"LightGBM\",\n",
    "        f\"lightgbm_experiment_{i}\",\n",
    "        config,\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Seleccionar mejor modelo y registrar en Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor run ID: 50567bfe44044e61bfa6db5ff380ee7a\n",
      "Modelo: SVM\n",
      "F1 Score: 1.0000\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Buscar el mejor run basado en F1 score\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"\",\n",
    "    order_by=[\"metrics.f1_score DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "if len(runs) > 0:\n",
    "    best_run = runs.iloc[0]\n",
    "    print(f\"Mejor run ID: {best_run['run_id']}\")\n",
    "    print(f\"Modelo: {best_run['params.model_type']}\")\n",
    "    print(f\"F1 Score: {best_run['metrics.f1_score']:.4f}\")\n",
    "    print(f\"Accuracy: {best_run['metrics.accuracy']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron runs exitosos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2db3ce59f2478a8b5945ae0b2a3160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee688b73bfa040e080089545006a90ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'penguins-classifier' already exists. Creating a new version of this model...\n",
      "2025/09/22 23:45:17 WARNING mlflow.tracking._model_registry.fluent: Run with id 50567bfe44044e61bfa6db5ff380ee7a has no artifacts at artifact path 'model', registering model based on models:/m-b349c7d733c94cc9bc1f38423d3fc345 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo encontrado, procediendo a registrar...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/22 23:45:17 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: penguins-classifier, version 2\n",
      "Created version '2' of model 'penguins-classifier'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo registrado: penguins-classifier\n",
      "Versi√≥n del modelo: 2\n",
      "‚úÖ Modelo penguins-classifier v2 promovido a Production\n"
     ]
    }
   ],
   "source": [
    "# Registrar el mejor modelo si existe\n",
    "if len(runs) > 0 and 'artifact_uri' in best_run:\n",
    "    model_name = \"penguins-classifier\"\n",
    "    model_uri = f\"runs:/{best_run['run_id']}/model\"\n",
    "    \n",
    "    # Verificar si el modelo tiene artefactos guardados\n",
    "    try:\n",
    "        # Intentar cargar el modelo para verificar que existe\n",
    "        test_model = mlflow.pyfunc.load_model(model_uri)\n",
    "        print(\"‚úÖ Modelo encontrado, procediendo a registrar...\")\n",
    "        \n",
    "        # Registrar modelo\n",
    "        try:\n",
    "            mlflow.register_model(model_uri, model_name)\n",
    "            print(f\"‚úÖ Modelo registrado: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Modelo ya existe o error: {e}\")\n",
    "        \n",
    "        # Obtener versi√≥n del modelo\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        model_version = client.get_latest_versions(model_name, stages=[\"None\"])[0]\n",
    "        print(f\"Versi√≥n del modelo: {model_version.version}\")\n",
    "        \n",
    "        # Transicionar modelo a Production\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=model_version.version,\n",
    "            stage=\"Production\",\n",
    "            archive_existing_versions=True\n",
    "        )\n",
    "        \n",
    "        # A√±adir descripci√≥n al modelo\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=model_version.version,\n",
    "            description=f\"Mejor modelo para clasificaci√≥n de ping√ºinos. F1={best_run['metrics.f1_score']:.4f}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Modelo {model_name} v{model_version.version} promovido a Production\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå El modelo no tiene artefactos guardados o no se puede cargar: {e}\")\n",
    "        print(\"   Esto puede deberse a problemas con el bucket S3.\")\n",
    "        print(\"   Los experimentos se ejecutaron pero los modelos no se pudieron guardar.\")\n",
    "else:\n",
    "    print(\"‚ùå No hay modelos disponibles para registrar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verificar modelo en Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76dd37ab5d549e4b79ea34546553da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicci√≥n de prueba:\n",
      "Input: [  44.5   17.1  200.  4200. ]\n",
      "Predicci√≥n: Adelie (c√≥digo: 0)\n"
     ]
    }
   ],
   "source": [
    "# Intentar cargar modelo desde Production si existe\n",
    "try:\n",
    "    model_uri = f\"models:/{model_name}/Production\"\n",
    "    loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "    \n",
    "    # Hacer predicci√≥n de prueba\n",
    "    test_input = pd.DataFrame({\n",
    "        'bill_length_mm': [44.5],\n",
    "        'bill_depth_mm': [17.1],\n",
    "        'flipper_length_mm': [200],\n",
    "        'body_mass_g': [4200]\n",
    "    })\n",
    "    \n",
    "    prediction = loaded_model.predict(test_input)\n",
    "    predicted_species = species_mapping[prediction[0]]\n",
    "    \n",
    "    print(f\"Predicci√≥n de prueba:\")\n",
    "    print(f\"Input: {test_input.values[0]}\")\n",
    "    print(f\"Predicci√≥n: {predicted_species} (c√≥digo: {prediction[0]})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå No se pudo cargar el modelo desde Production: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä RESUMEN FINAL:\n",
      "- Total de experimentos realizados: 40\n",
      "- Mejor modelo: SVM\n",
      "- F1 Score: 1.0000\n",
      "- Modelo registrado: penguins-classifier\n",
      "- Versi√≥n en Production: 2\n",
      "\n",
      "‚úÖ Pipeline MLflow completado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# Resumen final\n",
    "total_runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "print(f\"\\nüìä RESUMEN FINAL:\")\n",
    "print(f\"- Total de experimentos realizados: {len(total_runs)}\")\n",
    "\n",
    "if len(runs) > 0:\n",
    "    print(f\"- Mejor modelo: {best_run['params.model_type']}\")\n",
    "    print(f\"- F1 Score: {best_run['metrics.f1_score']:.4f}\")\n",
    "    \n",
    "    try:\n",
    "        if 'model_version' in locals():\n",
    "            print(f\"- Modelo registrado: {model_name}\")\n",
    "            print(f\"- Versi√≥n en Production: {model_version.version}\")\n",
    "            print(f\"\\n‚úÖ Pipeline MLflow completado exitosamente!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Pipeline MLflow completado parcialmente\")\n",
    "            print(\"   Los experimentos se ejecutaron pero hubo problemas con el registro del modelo\")\n",
    "    except:\n",
    "        print(f\"\\n‚ö†Ô∏è  Pipeline MLflow completado con advertencias\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No se completaron experimentos exitosamente\")\n",
    "\n",
    "if not bucket_ready:\n",
    "    print(\"\\n‚ö†Ô∏è  NOTA: Hubo problemas con el bucket S3.\")\n",
    "    print(\"   Los modelos pueden no haberse guardado correctamente.\")\n",
    "    print(\"   Verifica la configuraci√≥n de MinIO y vuelve a ejecutar el notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
