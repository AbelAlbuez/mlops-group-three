{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêß Experimentos MLflow - Palmer Penguins\n",
    "\n",
    "Este notebook implementa:\n",
    "1. Carga de datos en MySQL (penguins_raw)\n",
    "2. Preprocesamiento y generaci√≥n de datos limpios (penguins_clean)\n",
    "3. ‚â•20 experimentos con diferentes modelos e hiperpar√°metros\n",
    "4. Registro del mejor modelo en MLflow Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Patch aplicado para compatibilidad con SQLAlchemy 2.0\n"
     ]
    }
   ],
   "source": [
    "# Fix de compatibilidad para SQLAlchemy 2.0\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text as sql_text\n",
    "\n",
    "# Monkey patch para compatibilidad\n",
    "original_execute = sqlalchemy.engine.Connection.execute\n",
    "\n",
    "def patched_execute(self, statement, *args, **kwargs):\n",
    "    if isinstance(statement, str):\n",
    "        statement = sql_text(statement)\n",
    "    return original_execute(self, statement, *args, **kwargs)\n",
    "\n",
    "sqlalchemy.engine.Connection.execute = patched_execute\n",
    "\n",
    "print(\"‚úÖ Patch aplicado para compatibilidad con SQLAlchemy 2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: http://mlflow:5000\n"
     ]
    }
   ],
   "source": [
    "# Importaciones necesarias\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine, text\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000'))\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de conexi√≥n a MySQL\n",
    "MYSQL_CONFIG = {\n",
    "    'host': os.getenv('MYSQL_HOST', 'mysql'),\n",
    "    'port': int(os.getenv('MYSQL_PORT', 3306)),\n",
    "    'user': os.getenv('MYSQL_USER', 'penguins'),\n",
    "    'password': os.getenv('MYSQL_PASSWORD', 'penguins123'),\n",
    "    'database': os.getenv('MYSQL_DATABASE', 'penguins_db')\n",
    "}\n",
    "\n",
    "# Crear engine de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{MYSQL_CONFIG['user']}:{MYSQL_CONFIG['password']}@\"\n",
    "    f\"{MYSQL_CONFIG['host']}:{MYSQL_CONFIG['port']}/{MYSQL_CONFIG['database']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar datos crudos en MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado desde palmerpenguins\n",
      "Shape: (344, 8)\n",
      "Columnas: ['species', 'island', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex', 'year']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  year  \n",
       "0       3750.0    male  2007  \n",
       "1       3800.0  female  2007  \n",
       "2       3250.0  female  2007  \n",
       "3          NaN     NaN  2007  \n",
       "4       3450.0  female  2007  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar dataset de penguins\n",
    "try:\n",
    "    from palmerpenguins import load_penguins\n",
    "    df_raw = load_penguins()\n",
    "    print(\"Dataset cargado desde palmerpenguins\")\n",
    "except:\n",
    "    import seaborn as sns\n",
    "    df_raw = sns.load_dataset('penguins')\n",
    "    print(\"Dataset cargado desde seaborn\")\n",
    "\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(f\"Columnas: {df_raw.columns.tolist()}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla penguins_raw limpiada\n",
      "‚úì 344 registros insertados en penguins_raw\n"
     ]
    }
   ],
   "source": [
    "# Limpiar tabla penguins_raw\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"TRUNCATE TABLE penguins_raw\")\n",
    "    conn.commit()\n",
    "print(\"Tabla penguins_raw limpiada\")\n",
    "\n",
    "# Insertar datos crudos\n",
    "df_raw.to_sql('penguins_raw', engine, if_exists='append', index=False)\n",
    "print(f\"‚úì {len(df_raw)} registros insertados en penguins_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesar datos y generar penguins_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos le√≠dos: (344, 10)\n",
      "Despu√©s de eliminar nulos: (342, 10)\n",
      "Mapeo de especies: {0: 'Adelie', 1: 'Chinstrap', 2: 'Gentoo'}\n"
     ]
    }
   ],
   "source": [
    "# Leer datos desde MySQL\n",
    "df_from_db = pd.read_sql(\"SELECT * FROM penguins_raw\", engine)\n",
    "print(f\"Datos le√≠dos: {df_from_db.shape}\")\n",
    "\n",
    "# Eliminar filas con valores nulos en features cr√≠ticas\n",
    "critical_features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'species']\n",
    "df_clean = df_from_db.dropna(subset=critical_features)\n",
    "print(f\"Despu√©s de eliminar nulos: {df_clean.shape}\")\n",
    "\n",
    "# Codificar variable objetivo\n",
    "label_encoder = LabelEncoder()\n",
    "df_clean['species_encoded'] = label_encoder.fit_transform(df_clean['species'])\n",
    "species_mapping = {i: sp for i, sp in enumerate(label_encoder.classes_)}\n",
    "print(f\"Mapeo de especies: {species_mapping}\")\n",
    "\n",
    "# Rellenar valores faltantes opcionales\n",
    "df_clean['sex'] = df_clean['sex'].fillna('Unknown')\n",
    "df_clean['year'] = df_clean['year'].fillna(df_clean['year'].median())\n",
    "\n",
    "# Agregar timestamp\n",
    "df_clean['processed_at'] = pd.Timestamp.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì 342 registros insertados en penguins_clean\n"
     ]
    }
   ],
   "source": [
    "# Limpiar e insertar en penguins_clean\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"TRUNCATE TABLE penguins_clean\")\n",
    "    conn.commit()\n",
    "\n",
    "df_clean.to_sql('penguins_clean', engine, if_exists='append', index=False)\n",
    "print(f\"‚úì {len(df_clean)} registros insertados en penguins_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparar datos para Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (273, 4)\n",
      "Test set: (69, 4)\n",
      "Distribuci√≥n de clases en train: [121  54  98]\n"
     ]
    }
   ],
   "source": [
    "# Preparar features y target\n",
    "feature_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "X = df_clean[feature_cols].values\n",
    "y = df_clean['species_encoded'].values\n",
    "\n",
    "# Divisi√≥n train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Distribuci√≥n de clases en train: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimentos con MLflow (‚â•20 runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento: penguins-classification (ID: 1)\n"
     ]
    }
   ],
   "source": [
    "# Crear experimento en MLflow\n",
    "experiment_name = \"penguins-classification\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Obtener ID del experimento\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"Experimento: {experiment_name} (ID: {experiment.experiment_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos 1-5: Random Forest con diferentes hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/21 18:39:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run rf_experiment_1 at: http://mlflow:5000/#/experiments/1/runs/135e82d187664aa090c12fcc2b2256b2\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/1\n"
     ]
    },
    {
     "ename": "S3UploadFailedError",
     "evalue": "Failed to upload /tmp/tmpwsokro_r/model/conda.yaml to mlflows3/artifacts/1/models/m-d5b5e804895a4785b6da31775e6e98e2/artifacts/conda.yaml: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchBucket\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/boto3/s3/transfer.py:373\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# client error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/s3transfer/futures.py:111\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/s3transfer/futures.py:287\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/s3transfer/tasks.py:142\u001b[0m, in \u001b[0;36mTask.__call__\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/s3transfer/tasks.py:165\u001b[0m, in \u001b[0;36mTask._execute_main\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs_to_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# If the task is the final task, then set the TransferFuture's\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# value to the return value from main().\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/s3transfer/upload.py:796\u001b[0m, in \u001b[0;36mPutObjectTask._main\u001b[0;34m(self, client, fileobj, bucket, key, extra_args)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fileobj \u001b[38;5;28;01mas\u001b[39;00m body:\n\u001b[0;32m--> 796\u001b[0m     \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:602\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1078\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1078\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNoSuchBucket\u001b[0m: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mS3UploadFailedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, f1)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Log model\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                        \u001b[49m\u001b[43minput_example\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRF Exp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/sklearn/__init__.py:426\u001b[0m, in \u001b[0;36mlog_model\u001b[0;34m(sk_model, artifact_path, conda_env, code_paths, serialization_format, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, pyfunc_predict_fn, metadata, params, tags, model_type, step, model_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;129m@format_docstring\u001b[39m(LOG_MODEL_PARAM_DOCS\u001b[38;5;241m.\u001b[39mformat(package_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_model\u001b[39m(\n\u001b[1;32m    336\u001b[0m     sk_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    356\u001b[0m ):\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    Log a scikit-learn model as an MLflow artifact for the current run. Produces an MLflow Model\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    containing the following flavors:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msklearn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43msk_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msk_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconda_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconda_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserialization_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserialization_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_example\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyfunc_predict_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyfunc_predict_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/models/model.py:1302\u001b[0m, in \u001b[0;36mModel.log\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;66;03m# mlflow_model is updated, rewrite the MLmodel file\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m     mlflow_model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_path, MLMODEL_FILE_NAME))\n\u001b[0;32m-> 1302\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_model_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;66;03m# If the model was previously identified as external, delete the tag because\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# the model now has artifacts in MLflow Model format\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtags\u001b[38;5;241m.\u001b[39mget(MLFLOW_MODEL_IS_EXTERNAL, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/tracking/client.py:5589\u001b[0m, in \u001b[0;36mMlflowClient.log_model_artifacts\u001b[0;34m(self, model_id, local_dir)\u001b[0m\n\u001b[1;32m   5578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_model_artifacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_id: \u001b[38;5;28mstr\u001b[39m, local_dir: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    Upload a set of artifacts to the specified logged model.\u001b[39;00m\n\u001b[1;32m   5581\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5587\u001b[0m \u001b[38;5;124;03m        None\u001b[39;00m\n\u001b[1;32m   5588\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_model_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/client.py:910\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_model_artifacts\u001b[0;34m(self, model_id, local_dir)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_model_artifacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_id: \u001b[38;5;28mstr\u001b[39m, local_dir: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 910\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_artifact_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogged_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/store/artifact/s3_artifact_repo.py:299\u001b[0m, in \u001b[0;36mS3ArtifactRepository.log_artifacts\u001b[0;34m(self, local_dir, artifact_path)\u001b[0m\n\u001b[1;32m    296\u001b[0m     upload_path \u001b[38;5;241m=\u001b[39m posixpath\u001b[38;5;241m.\u001b[39mjoin(dest_path, rel_path)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms3_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposixpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mlflow/store/artifact/s3_artifact_repo.py:247\u001b[0m, in \u001b[0;36mS3ArtifactRepository._upload_file\u001b[0;34m(self, s3_client, local_file, bucket, key)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m environ_extra_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     extra_args\u001b[38;5;241m.\u001b[39mupdate(environ_extra_args)\n\u001b[0;32m--> 247\u001b[0m \u001b[43ms3_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mExtraArgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/boto3/s3/inject.py:175\u001b[0m, in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Upload a file to an S3 object.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/boto3/s3/transfer.py:379\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# client error.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m S3UploadFailedError(\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to upload \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    381\u001b[0m             filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([bucket, key]), e\n\u001b[1;32m    382\u001b[0m         )\n\u001b[1;32m    383\u001b[0m     )\n",
      "\u001b[0;31mS3UploadFailedError\u001b[0m: Failed to upload /tmp/tmpwsokro_r/model/conda.yaml to mlflows3/artifacts/1/models/m-d5b5e804895a4785b6da31775e6e98e2/artifacts/conda.yaml: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist"
     ]
    }
   ],
   "source": [
    "# Configuraciones de Random Forest\n",
    "rf_configs = [\n",
    "    {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5},\n",
    "    {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2},\n",
    "    {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2},\n",
    "    {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 4},\n",
    "    {'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 3}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(rf_configs, 1):\n",
    "    with mlflow.start_run(run_name=f\"rf_experiment_{i}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(config)\n",
    "        mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "        \n",
    "        # Crear y entrenar modelo\n",
    "        rf = RandomForestClassifier(random_state=42, **config)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = rf.predict(X_test)\n",
    "        \n",
    "        # M√©tricas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(rf, \"model\", \n",
    "                                input_example=X_train[:1],\n",
    "                                signature=mlflow.models.infer_signature(X_train, y_train))\n",
    "        \n",
    "        print(f\"RF Exp {i}: Accuracy={accuracy:.4f}, F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos 6-10: KNN con diferentes valores de k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN experiments\n",
    "k_values = [3, 5, 7, 10, 15]\n",
    "weights_options = ['uniform', 'distance']\n",
    "\n",
    "exp_num = 6\n",
    "for k in k_values:\n",
    "    for weights in weights_options[:1]:  # Solo 'uniform' para llegar a 5 experimentos\n",
    "        with mlflow.start_run(run_name=f\"knn_experiment_{exp_num}\"):\n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"model_type\", \"KNN\")\n",
    "            mlflow.log_param(\"n_neighbors\", k)\n",
    "            mlflow.log_param(\"weights\", weights)\n",
    "            \n",
    "            # Crear pipeline con escalado\n",
    "            knn_pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=k, weights=weights))\n",
    "            ])\n",
    "            \n",
    "            # Entrenar\n",
    "            knn_pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predicciones\n",
    "            y_pred = knn_pipeline.predict(X_test)\n",
    "            \n",
    "            # M√©tricas\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            \n",
    "            # Log model\n",
    "            mlflow.sklearn.log_model(knn_pipeline, \"model\",\n",
    "                                    input_example=X_train[:1],\n",
    "                                    signature=mlflow.models.infer_signature(X_train, y_train))\n",
    "            \n",
    "            print(f\"KNN Exp {exp_num}: k={k}, Accuracy={accuracy:.4f}, F1={f1:.4f}\")\n",
    "            exp_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos 11-15: SVM con diferentes kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM experiments\n",
    "svm_configs = [\n",
    "    {'kernel': 'linear', 'C': 0.1},\n",
    "    {'kernel': 'linear', 'C': 1.0},\n",
    "    {'kernel': 'rbf', 'C': 1.0, 'gamma': 'scale'},\n",
    "    {'kernel': 'rbf', 'C': 10.0, 'gamma': 'auto'},\n",
    "    {'kernel': 'poly', 'C': 1.0, 'degree': 3}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(svm_configs, 11):\n",
    "    with mlflow.start_run(run_name=f\"svm_experiment_{i}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(config)\n",
    "        mlflow.log_param(\"model_type\", \"SVM\")\n",
    "        \n",
    "        # Crear pipeline con escalado\n",
    "        svm_pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svm', SVC(probability=True, random_state=42, **config))\n",
    "        ])\n",
    "        \n",
    "        # Entrenar\n",
    "        svm_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = svm_pipeline.predict(X_test)\n",
    "        \n",
    "        # M√©tricas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(svm_pipeline, \"model\",\n",
    "                                input_example=X_train[:1],\n",
    "                                signature=mlflow.models.infer_signature(X_train, y_train))\n",
    "        \n",
    "        print(f\"SVM Exp {i}: kernel={config['kernel']}, Accuracy={accuracy:.4f}, F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos 16-20: XGBoost con diferentes configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost experiments\n",
    "xgb_configs = [\n",
    "    {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1},\n",
    "    {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.05},\n",
    "    {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.01},\n",
    "    {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.1},\n",
    "    {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.3}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(xgb_configs, 16):\n",
    "    with mlflow.start_run(run_name=f\"xgboost_experiment_{i}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(config)\n",
    "        mlflow.log_param(\"model_type\", \"XGBoost\")\n",
    "        \n",
    "        # Crear modelo\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            random_state=42,\n",
    "            **config\n",
    "        )\n",
    "        \n",
    "        # Entrenar\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        \n",
    "        # M√©tricas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(xgb_model, \"model\",\n",
    "                                input_example=X_train[:1],\n",
    "                                signature=mlflow.models.infer_signature(X_train, y_train))\n",
    "        \n",
    "        print(f\"XGB Exp {i}: Accuracy={accuracy:.4f}, F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos adicionales 21-25: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM experiments para completar ‚â•20\n",
    "lgb_configs = [\n",
    "    {'n_estimators': 100, 'num_leaves': 31, 'learning_rate': 0.1},\n",
    "    {'n_estimators': 200, 'num_leaves': 50, 'learning_rate': 0.05},\n",
    "    {'n_estimators': 150, 'num_leaves': 20, 'learning_rate': 0.15},\n",
    "    {'n_estimators': 300, 'num_leaves': 40, 'learning_rate': 0.01},\n",
    "    {'n_estimators': 250, 'num_leaves': 60, 'learning_rate': 0.08}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(lgb_configs, 21):\n",
    "    with mlflow.start_run(run_name=f\"lightgbm_experiment_{i}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(config)\n",
    "        mlflow.log_param(\"model_type\", \"LightGBM\")\n",
    "        \n",
    "        # Crear modelo\n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "            objective='multiclass',\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            **config\n",
    "        )\n",
    "        \n",
    "        # Entrenar\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = lgb_model.predict(X_test)\n",
    "        \n",
    "        # M√©tricas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(lgb_model, \"model\",\n",
    "                                input_example=X_train[:1],\n",
    "                                signature=mlflow.models.infer_signature(X_train, y_train))\n",
    "        \n",
    "        print(f\"LGB Exp {i}: Accuracy={accuracy:.4f}, F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Seleccionar mejor modelo y registrar en Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar el mejor run basado en F1 score\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"\",\n",
    "    order_by=[\"metrics.f1_score DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "best_run = runs.iloc[0]\n",
    "print(f\"Mejor run ID: {best_run['run_id']}\")\n",
    "print(f\"Modelo: {best_run['params.model_type']}\")\n",
    "print(f\"F1 Score: {best_run['metrics.f1_score']:.4f}\")\n",
    "print(f\"Accuracy: {best_run['metrics.accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar el mejor modelo\n",
    "model_name = \"penguins-classifier\"\n",
    "model_uri = f\"runs:/{best_run['run_id']}/model\"\n",
    "\n",
    "# Registrar modelo\n",
    "try:\n",
    "    # Intentar crear el modelo registrado\n",
    "    mlflow.register_model(model_uri, model_name)\n",
    "    print(f\"‚úì Modelo registrado: {model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Modelo ya existe o error: {e}\")\n",
    "\n",
    "# Obtener versi√≥n del modelo\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "model_version = client.get_latest_versions(model_name, stages=[\"None\"])[0]\n",
    "print(f\"Versi√≥n del modelo: {model_version.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transicionar modelo a Production\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True\n",
    ")\n",
    "\n",
    "# A√±adir descripci√≥n al modelo\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    description=f\"Mejor modelo para clasificaci√≥n de ping√ºinos. F1={best_run['metrics.f1_score']:.4f}\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Modelo {model_name} v{model_version.version} promovido a Production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verificar modelo en Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo desde Production\n",
    "model_uri = f\"models:/{model_name}/Production\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Hacer predicci√≥n de prueba\n",
    "test_input = pd.DataFrame({\n",
    "    'bill_length_mm': [44.5],\n",
    "    'bill_depth_mm': [17.1],\n",
    "    'flipper_length_mm': [200],\n",
    "    'body_mass_g': [4200]\n",
    "})\n",
    "\n",
    "prediction = loaded_model.predict(test_input)\n",
    "predicted_species = species_mapping[prediction[0]]\n",
    "\n",
    "print(f\"Predicci√≥n de prueba:\")\n",
    "print(f\"Input: {test_input.values[0]}\")\n",
    "print(f\"Predicci√≥n: {predicted_species} (c√≥digo: {prediction[0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen final\n",
    "total_runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "print(f\"\\nüìä RESUMEN FINAL:\")\n",
    "print(f\"- Total de experimentos realizados: {len(total_runs)}\")\n",
    "print(f\"- Mejor modelo: {best_run['params.model_type']}\")\n",
    "print(f\"- F1 Score: {best_run['metrics.f1_score']:.4f}\")\n",
    "print(f\"- Modelo registrado: {model_name}\")\n",
    "print(f\"- Versi√≥n en Production: {model_version.version}\")\n",
    "print(f\"\\n‚úÖ Pipeline MLflow completado exitosamente!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
