# ðŸ§ Taller 3 â€“ Airflow con Docker Compose y API de Inferencia

## ðŸ“Œ DescripciÃ³n
Este taller implementa un **pipeline de Machine Learning para clasificaciÃ³n de pingÃ¼inos** usando **Airflow + Docker Compose**.  

El sistema incluye:  
- **Airflow**: DAG que limpia la BD, carga datos, preprocesa, entrena modelos y valida el pipeline.  
- **MySQL**: base de datos de datos (`penguins_raw`, `penguins_clean`).  
- **Postgres**: metadatos de Airflow.  
- **API FastAPI**: expone modelos entrenados para realizar inferencias.  

---

## ðŸ‘¥ Roles en el equipo
- **Persona 1**: Infraestructura y orquestaciÃ³n (Docker Compose, DBs, Airflow, API).  
- **Persona 2**: DAG de Airflow (pipeline completo).  
- **Persona 3**: API de inferencia (FastAPI).  

---

## ðŸ“‚ Estructura del proyecto

```
Taller-3/
â”œâ”€ airflow/                       # ConfiguraciÃ³n de Airflow
â”‚  â”œâ”€ dags/                       # DAGs de Airflow (pipeline de pingÃ¼inos)
â”‚  â”œâ”€ logs/                       # Logs de ejecuciÃ³n de Airflow
â”‚  â”œâ”€ models/                     # Modelos sklearn (.pkl, metadata)
â”‚  â”œâ”€ models_tf/                  # Modelos TensorFlow (.h5, SavedModel)
â”‚  â”œâ”€ plugins/                    # Plugins personalizados
â”‚  â””â”€ requirements.txt            # Dependencias adicionales para DAGs
â”œâ”€ api/                           # API de inferencia
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ api.py                      # API FastAPI (modelos sklearn)
â”‚  â”œâ”€ api_tf.py                   # API FastAPI (modelos TensorFlow)
â”‚  â”œâ”€ Dockerfile                  # Imagen de la API
â”‚  â””â”€ requirements.txt            # Dependencias de la API
â”œâ”€ data/                          # Datos del proyecto
â”‚  â”œâ”€ raw/                        # Datos crudos
â”‚  â””â”€ processed/                   # Datos procesados
â”œâ”€ mysql/                         # Persistencia de MySQL
â”œâ”€ venv/                          # Entorno virtual local (no versionar)
â”œâ”€ .env                           # Variables de entorno (no versionar)
â”œâ”€ .env.example                   # Ejemplo de configuraciÃ³n de entorno
â”œâ”€ docker-compose.yml             # OrquestaciÃ³n de servicios
â”œâ”€ README.md                      # DocumentaciÃ³n principal
â”œâ”€ requirements-local.txt         # Dependencias para pruebas locales
â””â”€ .gitignore
```

---

## âš™ï¸ Requisitos
- Linux (mÃ¡quina de la universidad).  
- **Docker** y **Docker Compose** instalados.  
- **Python 3.11** (para entorno virtual local).  

---

## ðŸ ConfiguraciÃ³n local con venv (opcional)
Para probar localmente sin Docker:

1. Crear entorno virtual:
```bash
python3 -m venv venv
source venv/bin/activate
```

2. Instalar dependencias:
```bash
pip install --upgrade pip
pip install -r requirements-local.txt
```

3. Entrenar modelos localmente:
```bash
python train_models.py --out ./airflow/models
```

4. Levantar la API localmente:
```bash
uvicorn api.api:app --reload --port 8000
```

---

## ðŸ³ Uso con Docker Compose
1. Copiar `.env.example` a `.env` y configurar credenciales (FERNET_KEY, MySQL, Postgres, Airflow admin).  

2. Construir e iniciar servicios:
```bash
docker compose up --build
```

3. Acceder a:
- Airflow UI â†’ [http://localhost:8080](http://localhost:8080)  
- API FastAPI â†’ [http://localhost:8000/docs](http://localhost:8000/docs)  

---

## ðŸš€ Flujo del DAG (`penguins_ml_pipeline`)
1. **clear_database** â†’ limpia tablas `penguins_raw` y `penguins_clean`.  
2. **load_raw_data** â†’ carga dataset crudo de pingÃ¼inos.  
3. **preprocess_data** â†’ limpia nulos, codifica variables y genera metadata.  
4. **train_models** â†’ entrena KNN, Random Forest y SVM; guarda el mejor como `model.pkl`.  
5. **validate_pipeline** â†’ valida que todo se ejecutÃ³ correctamente.  

---
## âš¡EjecuciÃ³n del DAG
- Se crea una nueva conexiÃ³n `mysql_penguins` en `AirFlow > Admin > Connections`.
  
  ![Connections](https://raw.githubusercontent.com/AbelAlbuez/mlops-group-three/refs/heads/main/Taller-3/images/1_connections.png)

- Se dan permisos a la carpeta `models` en el proyecto de Airflow:

```bash
sudo mkdir -p ./airflow/models
sudo chown -R 50000:0 ./airflow/models
sudo chmod -R 775 ./airflow/models```

- Se visualizan los 5 pasos definidos en el **DAG** que conforman el pipeline.

  ![Pipeline](https://raw.githubusercontent.com/AbelAlbuez/mlops-group-three/refs/heads/main/Taller-3/images/2_pipeline_graph.png)
  
- Las tablas que hacen parte del esquema.

  ![Schema](https://raw.githubusercontent.com/AbelAlbuez/mlops-group-three/refs/heads/main/Taller-3/images/3_database_tables.png)

- Las tablas vacias antes de la ejecuciÃ³n del **DAG**.

  ![Tables](https://raw.githubusercontent.com/AbelAlbuez/mlops-group-three/refs/heads/main/Taller-3/images/4_tables_count.png)

- **Se ejecuta el DAG**.
- Se observa la ejecuciÃ³n correcta del Pipeline.

![PipelineOK](https://raw.githubusercontent.com/AbelAlbuez/mlops-group-three/refs/heads/main/Taller-3/images/5_pipeline_graph_ok.png)

- Tareas ejecutadas correctamente.

![Task](https://raw.githubusercontent.com/AbelAlbuez/mlops-group-three/refs/heads/main/Taller-3/images/6_succes_exec.png)

- Calendario de ejecuciÃ³n

![Calendar](https://raw.githubusercontent.com/AbelAlbuez/mlops-group-three/refs/heads/main/Taller-3/images/7_calendar.png)

- Las tablas ya muestran registros cargados.

![Tables](https://raw.githubusercontent.com/AbelAlbuez/mlops-group-three/refs/heads/main/Taller-3/images/8_table_count_2.png)

- Las tablas ya muestran datos.

![Tables2](https://raw.githubusercontent.com/AbelAlbuez/mlops-group-three/refs/heads/main/Taller-3/images/9_tables.png)

- Los modelos han sido actualizados.

![Models](https://raw.githubusercontent.com/AbelAlbuez/mlops-group-three/refs/heads/main/Taller-3/images/10_models.png)



---

## ðŸ“¡ Endpoints de la API
- **GET /health** â†’ estado de la API y modelos cargados.  
- **GET /models** â†’ lista de modelos disponibles.  
- **POST /predict** â†’ predicciÃ³n con todos los modelos.  
- **POST /predict/{model_name}** â†’ predicciÃ³n con un modelo especÃ­fico.  

Ejemplo:
```bash
curl -X POST "http://localhost:8000/predict"   -H "Content-Type: application/json"   -d '{"bill_length_mm":44.5,"bill_depth_mm":17.1,"flipper_length_mm":200,"body_mass_g":4200}'
```

---

## âœ… Checklist de validaciÃ³n
- [ ] Postgres y MySQL levantan y estÃ¡n `healthy`.  
- [ ] Airflow inicializa y crea usuario admin.  
- [ ] Airflow UI accesible en `:8080`.  
- [ ] API accesible en `:8000`.  
- [ ] DAG ejecuta y genera `model.pkl` en `airflow/models`.  
- [ ] API lista modelos y responde predicciones.  

---

## ðŸ“Š Diagrama (Mermaid)

```mermaid
flowchart LR
    subgraph Airflow
        A[clear_database] --> B[load_raw_data]
        B --> C[preprocess_data]
        C --> D[train_models]
        D --> E[validate_pipeline]
    end

    subgraph Databases
        MySQL[(MySQL: Datos)]
        Postgres[(Postgres: Metadatos Airflow)]
    end

    Airflow --> MySQL
    Airflow --> Postgres
    Airflow -->|model.pkl| API
    API --> Cliente[Usuario / Cliente externo]
```

---

## ðŸ”§ Comandos Ãºtiles
- Ver logs de Airflow:
```bash
docker compose logs -f airflow-webserver
```

- Entrar al contenedor de MySQL:
```bash
docker compose exec mysql mysql -u root -p
```

- Entrar al contenedor de Airflow:
```bash
docker compose exec airflow-webserver bash
```

- Reconstruir imÃ¡genes y reiniciar:
```bash
docker compose up --build --force-recreate
```

---

ðŸ‘‰ Con esta documentaciÃ³n, cualquier miembro del equipo puede levantar el Taller 3 en la **mÃ¡quina Linux de la universidad** y validar todo el flujo end-to-end.
