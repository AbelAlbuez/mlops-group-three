version: '3.8'

services:
  # API de inferencia usando la imagen oficial ogaston/inference-g3:latest
  inference-api:
    image: ogaston/inference-g3:latest
    platform: linux/amd64
    container_name: inference-api-simple
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
      # ConfiguraciÃ³n para funcionar sin MLflow
      - MLFLOW_TRACKING_URI=http://localhost:5000
      - MYSQL_HOST=localhost
      - MYSQL_USER=test
      - MYSQL_PASSWORD=test
      - MYSQL_DATABASE=test
    deploy:
      resources:
        limits:
          cpus: '${API_CPU_LIMIT:-0.5}'
          memory: ${API_MEMORY_LIMIT:-512M}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - locust-network

  # Locust Master
  locust-master:
    image: locustio/locust:latest
    container_name: locust-master-simple
    ports:
      - "8089:8089"
    volumes:
      - ./locustfile.py:/mnt/locust/locustfile.py:ro
    command: >
      --locustfile=/mnt/locust/locustfile.py
      --master
      --host=http://inference-api:8000
    depends_on:
      - inference-api
    networks:
      - locust-network

  # Locust Worker
  locust-worker:
    image: locustio/locust:latest
    volumes:
      - ./locustfile.py:/mnt/locust/locustfile.py:ro
    command: >
      --locustfile=/mnt/locust/locustfile.py
      --worker
      --master-host=locust-master
    depends_on:
      - locust-master
    networks:
      - locust-network

networks:
  locust-network:
    driver: bridge
