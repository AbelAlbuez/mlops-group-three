version: '3.8'

services:
  inference-api:
    image: ogaston/inference-g3:latest
    container_name: inference-api-locust-minimal
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        limits:
          cpus: '${API_CPU_LIMIT:-0.5}'
          memory: ${API_MEMORY_LIMIT:-512M}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - locust-network

  locust-master:
    image: locustio/locust:latest
    container_name: locust-master-minimal
    ports:
      - "8089:8089"
    volumes:
      - ./locustfile.py:/mnt/locust/locustfile.py:ro
    command: >
      --locustfile=/mnt/locust/locustfile.py
      --master
      --host=http://inference-api:8000
    depends_on:
      - inference-api
    networks:
      - locust-network

  locust-worker:
    image: locustio/locust:latest
    volumes:
      - ./locustfile.py:/mnt/locust/locustfile.py:ro
    command: >
      --locustfile=/mnt/locust/locustfile.py
      --worker
      --master-host=locust-master
    depends_on:
      - locust-master
    networks:
      - locust-network

networks:
  locust-network:
    driver: bridge
